{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d7a46b-21db-4f90-adee-a65bc5ecaf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\megha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd2101a-efa8-4e32-93f3-32e4f271e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The future of Artificial Intelligence is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8480439-350c-43a2-a9bc-cfbc03ce67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_bert = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
    "gen_bert(prompt, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113acab1-0bc6-45d5-a092-91a8cb4ffa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_roberta = pipeline(\"text-generation\", model=\"roberta-base\")\n",
    "gen_roberta(prompt, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d8cb45-c03a-4f6d-83dd-55ba8c95afc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 159/159 [00:00<00:00, 167.61it/s, Materializing param=model.decoder.layers.5.self_attn_layer_n\n",
      "This checkpoint seem corrupted. The tied weights mapping for this model specifies to tie model.decoder.embed_tokens.weight to lm_head.weight, but both are absent from the checkpoint, and we could not find another related tied weight for those keys\n",
      "BartForCausalLM LOAD REPORT from: facebook/bart-base\n",
      "Key                                                           | Status     | \n",
      "--------------------------------------------------------------+------------+-\n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.bias     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.bias       | UNEXPECTED | \n",
      "encoder.embed_positions.weight                                | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.bias       | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.out_proj.weight   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc2.bias                    | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.v_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.q_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.weight     | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.final_layer_norm.bias       | UNEXPECTED | \n",
      "shared.weight                                                 | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.weight                  | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.bias   | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.fc1.bias                    | UNEXPECTED | \n",
      "encoder.layernorm_embedding.weight                            | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn_layer_norm.weight | UNEXPECTED | \n",
      "encoder.layers.{0, 1, 2, 3, 4, 5}.self_attn.k_proj.bias       | UNEXPECTED | \n",
      "encoder.layernorm_embedding.bias                              | UNEXPECTED | \n",
      "lm_head.weight                                                | MISSING    | \n",
      "model.decoder.embed_tokens.weight                             | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The future of Artificial Intelligence isJSNETJan lightspick fighters small mashed huggedgrading mashedgrading mashedATTLE repay wanderingusha savings savingsgrading mashed mashed mashedSER mashed MAcharcharceive savings Chapel ResearchgradingSmgradinggrading mashed Research nas MA MA MA culmination surrender MA subtlySm asleep sul murky mashedSm culminationchar mashed mashed Redd culmination MA MAagall compe confrontation ChapelSmstorms MA MADialogue MAostic Research MA MASmSm MA groceries MA savings culmination MA Supplementary Didn MA MA693 Chapel MA MAospaceospaceSm harmon groceriesostic MASm MAgrading MAplain MA MAplain arous Chapelospace whites MAphalt MASm harmonSm Researchostic MAchar MA MA bending whitesSm RendercharSm MA Researchil MAplain Tweet MA MA 75 MA MAph MA Renderunct MASm culmination MA Renderphalt MA harmon launcher chieflySmDialoguegradinggrading waste MA MA harmonplain MASm launcher Stream launcher Nordic MA chieflygradingostic harmon MA MA chiefly gif MASmostic MA harmonphalt MAostic MA MA launcher chiefly chieflyphalt Vitalunct MA chiefly chiefly MASm chieflyphalt Research launchergradingSm MAunct Research chieflyphalt up chiefly chieflyño up chieflyphaltphaltGra launcher MA Vitalplain chiefly gif chieflySm launcherphaltuni chiefly Research chiefly chiefly chiefly693GrañoSmGra chieflySm chiefly recruitingGra MA recruiting chieflyGra recruiting chieflyño chiefly chieflyGraphalt aiding'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_bart = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
    "gen_bart(prompt, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd00e31-dab4-4a12-9328-246a262ef379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██| 202/202 [00:01<00:00, 155.36it/s, Materializing param=cls.predictions.transform.dense.weight]\n",
      "BertForMaskedLM LOAD REPORT from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Loading weights: 100%|█| 202/202 [00:01<00:00, 163.82it/s, Materializing param=roberta.encoder.layer.11.output.dense.we\n",
      "RobertaForMaskedLM LOAD REPORT from: roberta-base\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT: {'score': 0.5396933555603027, 'token': 3443, 'token_str': 'create', 'sequence': 'the goal of generative ai is to create new content.'}\n"
     ]
    },
    {
     "ename": "PipelineException",
     "evalue": "No mask_token (<mask>) found on the input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPipelineException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m mask_roberta \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill-mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERT:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mask_bert(mask_text)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoBERTa:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmask_roberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_text\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\fill_mask.py:256\u001b[0m, in \u001b[0;36mFillMaskPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[0;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m    Fill the masked token in the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m        - **token_str** (`str`) -- The predicted token (to replace the masked one).\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1274\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1267\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1268\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1271\u001b[0m         )\n\u001b[0;32m   1272\u001b[0m     )\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1280\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m-> 1280\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m   1281\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1282\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\fill_mask.py:117\u001b[0m, in \u001b[0;36mFillMaskPipeline.preprocess\u001b[1;34m(self, inputs, return_tensors, tokenizer_kwargs, **preprocess_parameters)\u001b[0m\n\u001b[0;32m    114\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    116\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(inputs, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_exactly_one_mask_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\fill_mask.py:106\u001b[0m, in \u001b[0;36mFillMaskPipeline.ensure_exactly_one_mask_token\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_exactly_one_mask_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\fill_mask.py:94\u001b[0m, in \u001b[0;36mFillMaskPipeline._ensure_exactly_one_mask_token\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m     92\u001b[0m numel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(masked_index\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numel \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineException(\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill-mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbase_model_prefix,\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo mask_token (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmask_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) found on the input\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     98\u001b[0m     )\n",
      "\u001b[1;31mPipelineException\u001b[0m: No mask_token (<mask>) found on the input"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_text = \"The goal of Generative AI is to [MASK] new content.\"\n",
    "\n",
    "mask_bert = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "mask_roberta = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
    "\n",
    "print(\"BERT:\", mask_bert(mask_text)[0])\n",
    "print(\"RoBERTa:\", mask_roberta(mask_text)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4c272c-aa8b-4697-aa8a-e1af6c76a36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 197/197 [00:01<00:00, 191.79it/s, Materializing param=bert.encoder.layer.11.output.dense.weigh\n",
      "BertForQuestionAnswering LOAD REPORT from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "bert.pooler.dense.weight                   | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "bert.pooler.dense.bias                     | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "qa_outputs.bias                            | MISSING    | \n",
      "qa_outputs.weight                          | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Loading weights: 100%|█| 197/197 [00:01<00:00, 166.86it/s, Materializing param=roberta.encoder.layer.11.output.dense.we\n",
      "RobertaForQuestionAnswering LOAD REPORT from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "qa_outputs.bias                 | MISSING    | \n",
      "qa_outputs.weight               | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n",
      "Loading weights: 100%|█████████████████████| 259/259 [00:02<00:00, 118.06it/s, Materializing param=model.shared.weight]\n",
      "BartForQuestionAnswering LOAD REPORT from: facebook/bart-base\n",
      "Key               | Status  | \n",
      "------------------+---------+-\n",
      "qa_outputs.bias   | MISSING | \n",
      "qa_outputs.weight | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT: {'score': 0.009075992507860065, 'start': 20, 'end': 60, 'answer': 'significant risks such as hallucinations'}\n",
      "RoBERTa: {'score': 0.011823982000350952, 'start': 66, 'end': 81, 'answer': ', and deepfakes'}\n",
      "BART: {'score': 0.028814487159252167, 'start': 60, 'end': 81, 'answer': ', bias, and deepfakes'}\n"
     ]
    }
   ],
   "source": [
    "qa_context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
    "qa_question = \"What are the risks?\"\n",
    "\n",
    "qa_bert = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
    "qa_roberta = pipeline(\"question-answering\", model=\"roberta-base\")\n",
    "qa_bart = pipeline(\"question-answering\", model=\"facebook/bart-base\")\n",
    "\n",
    "print(\"BERT:\", qa_bert(question=qa_question, context=qa_context))\n",
    "print(\"RoBERTa:\", qa_roberta(question=qa_question, context=qa_context))\n",
    "print(\"BART:\", qa_bart(question=qa_question, context=qa_context))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d561e-21e7-4b08-b471-f6e77a9f234f",
   "metadata": {},
   "source": [
    "| Task        | Model   | Classification (Success / Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
    "|-------------|---------|-----------------------------------|---------------------------------------|--------------------------------------------|\n",
    "| Generation  | BERT    | Failure                            | Model failed to generate meaningful text or stalled during execution. | BERT is an encoder-only model and is not trained for autoregressive text generation. |\n",
    "| Generation  | RoBERTa | Failure                            | Similar to BERT, output was incoherent or generation did not complete properly. | RoBERTa is also an encoder-only model focused on understanding, not generation. |\n",
    "| Generation  | BART    | Partial Success                    | Text was generated but was largely incoherent and accompanied by weight mismatch warnings. | BART is an encoder–decoder model, but it is not optimized for free-form text generation like GPT-style models. |\n",
    "| Fill-Mask   | BERT    | Success                            | Correctly predicted masked words such as “create” and “generate”. | BERT is trained using Masked Language Modeling (MLM). |\n",
    "| Fill-Mask   | RoBERTa | Success                            | Produced accurate and confident predictions for the masked token. | RoBERTa is optimized for MLM with improved training strategies. |\n",
    "| Fill-Mask   | BART    | Failure                            | Task not well supported; predictions were poor or inconsistent. | BART is trained for sequence-to-sequence tasks, not MLM. |\n",
    "| QA          | BERT    | Partial Success                    | Answer was incomplete or sometimes inaccurate. | Base BERT is not fine-tuned on QA datasets like SQuAD. |\n",
    "| QA          | RoBERTa | Partial Success                    | Slightly better answers than BERT but still inconsistent. | Improved pretraining helps, but QA fine-tuning is required. |\n",
    "| QA          | BART    | Failure                            | Output was incorrect or unrelated to the question. | BART requires task-specific QA fine-tuning; base model is unsuitable. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
